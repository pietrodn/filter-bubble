# Introduction

## A provoking quote

> “A squirrel dying in front of your house may be more relevant to your interests right now than people dying in Africa.”

> – Mark Zuckerberg

## What is the filter bubble?
![*A unique universe of information for each of us* [@pariser2011]](images/FilterBubble.png)

## Definitions

### Personalized search

\footnotesize

> **Personalized search** refers to search experiences that are tailored specifically to an individual's interests by incorporating information about the individual beyond specific query provided. (Wikipedia)

### Filter bubble

\footnotesize

> A **filter bubble** is a result of a personalized search in which a website algorithm selectively guesses what information a user would like to see based on information about the user (such as location, past click behavior and search history). (Wikipedia)

### Political pluralism in the media

\footnotesize

> **Political pluralism** in the media refers to the fair and diverse representation of and expression by (i.e. passive and active access) various political and ideological groups, including minority viewpoints and interests, in the media. [@leuven2009]

## Personalizing filters and information

In this presentation, I will show that:

> **Personalized content filters may be harmful for the plurality of information**, when citizens and readers are not fully aware of them

* filters alter our perception of the world
* information given to citizens should be not filtered according to what they like
* even if they seem fair, **these algorithms are not neutral**


# Concerns about the Filter Bubble

## The dangers of personalization

The book *The Filter Bubble* by Eli Pariser [@pariser2011] describes many risks associated with it:

* Data collection and privacy
* Democracy
* **Information** (I will focus on this)
* Freedom
* Creativity
* Censorship
* Serendipity


## Information

* **Friendly world syndrome**: some of the most important problem don't reach our view at all
    * We can miss major news and events
    * This is not possible with traditional newspapers
* In the filter bubble, the **public sphere** is less relevant
* Filters block **important, unpleasant things** that we *should* care about
    * Some topics will always be *not likable*: war, homelessness, poverty...
* *Relevance* is the only metric, and *importance* matters less
* The **choice** about what to read is no more in readers' hands
    * Readers are not exposed to contents inside their personal bubble
* Worst case scenario: **deliberate use of filters to shape the public opinion**

# Case study: Facebook (and Google)

## Facebook is too friendly!

Suppose that you are a Facebook user and you identify as a **liberal**, and you have both liberals and conservatives friends.

* News Feed recommendation algorithm: **you get more posts which reflect what you like**
* You may not see conservatives' stories at all, if you interact less with your conservative friends
* **Cross-cutting stories** (those different from our viewpoint) are less likely to reach us
    * How big is this phenomenon?

**89.4% of under-30 Italians** uses Facebook [@censis2016]

## Facebook: Exposure to ideologically diverse content

Facebook published a study [@bakshy2015] on *Science* about how likely are users to **view and interact with *cross-cutting content***.

![Exposure stages of news stories](images/ExposureStages.pdf){width=70%}

1. *Potential from network*: all the content shared by friends
2. *Exposed*: content effectively shown in users' News Feeds
3. *Selected*: content clicked by the user

## % cross-cutting content vs. exposure stage on Facebook

![](images/CrossCuttingTotal_flat.pdf){height=75%}

## Facebook study: conclusions

* The **friendship network** is the most important factor limiting the mix of content encountered in social media
    * if I have only friends of the same political affiliation, the filter bubble is obvious
* **Individual choice** influences the exposure to cross-cutting content more than the News Feed filtering
* The effect of **News Feed ranking** is limited:
    * -5% for liberals
    * -8% for conservatives

\alert{Thus, Facebook says, any "filter bubble" is not due to the News Feed selection algorithm}

## Facebook study: criticism

### Limitations of the study

* Underlying (false) assumption: the building of the **friendship network** is independent from Facebook's algorithms
    * Friends are only partly from "offline" connections
    * Facebook suggests both pages to like and new friends
* What about **sponsored content**?

### Methodological issues

* **Sample** of the study: people which declare their political affiliation
    * may not be representative of the entire Facebook community
* Independent researchers can't access Facebook data and analyze it

## The position is everything

The **ranking** of a story in the News Feed is very important!

* the position in the News Feed may be used to promote some stories and not others
* money can buy rankings!
* even if the algorithm is "fair" *now*, what about the future?

![Click rate depends on the position of the story in the News Feed.](images/PositionNewsFeed.pdf){width=100%}


# Proposed remedies and counter-objections

## Moralizing filters

Problem: the Internet is showing off what we *want* to see, but not what we *need* to see

* What if one day Google could urge us to stop obsessing over Lady Gaga's videos and instead pay attention to Darfur?
* Would it be a good idea to make multinational companies **moralizing agents**?
    * Monopoly of editorial power
    * **Paternalism**
* Algorithms cannot compute "what should be seen" [@morozov2011]

We need media that prioritizes **importance** over relevance, but:

* this requires human intervention and choice
* using an algorithm is still a non-neutral choice!


## Make the algorithms transparent

What if the **algorithms** and/or some of the **data** were **public**?

* The inner working of complex neural networks and machine learning agents is not intuitively understandable
    * Even if published, we may not understand those algorithms
* They are often **trade secrets**
* Knowing at least **which personal data** is used to make the recommendation may prove useful
* **Open-sourcing** some critical services may be a solution


## Facebook News Feed settings

![Facebook lets users customize some parameters of the News Feed algorithm](images/NewsFeedCustomize.png){width=80%}

## Government oversight

Problem: the personalizing filters are not regulated in a **democratic** way
Governments could banish personalization or control it in some form

* Information should be independent by the Government
* The convergence of power of multinational companies and governments is dangerous


## Turn off the personalization!

* What if we could **turn off the personalization**?
* Personalization is the key feature of most services
    * Facebook without personalization would be... Twitter?
* Without personalization **ads would be less relevant** and profitable: no economic incentive to do so
* Users should at least know **whether** personalization is enabled or not


# Conclusions

## Conclusions

* Personalization is **pervasive**
* Personalizing filters return a **distorted image** of the world
* They are a really good technology for many specific applications
    * "Too good" to get rid of them
    * Users should be aware of them
    * Users should be able to control them
* Biggest risk: **deliberate use of filters to shape the public opinion**
