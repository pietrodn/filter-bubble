# Introduction

## A provoking quote

> “A squirrel dying in front of your house may be more relevant to your interests right now than people dying in Africa.” – Mark Zuckerberg


## Definitions

> **Personalized search** refers to search experiences that are tailored specifically to an individual's interests by incorporating information about the individual beyond specific query provided. (Wikipedia)

> A **filter bubble** is a result of a personalized search in which a website algorithm selectively guesses what information a user would like to see based on information about the user (such as location, past click behavior and search history). (Wikipedia)


## Amazon's filter bubble
![A "real world" recommender system](images/recommender_shop.jpg){width=80%}


## Personalizing filters and information

In this presentation, I will show that:

> **Personalized search filters may be dangerous for information**, if they are applied without people knowing

* filters alter our perception of the world
* information given to citizens should be not filtered according to what they like
* even if they seem fair, **these algorithms are not neutral**


# Concerns about the Filter Bubble

## The dangers of personalization
The book *The Filter Bubble* by Eli Pariser [@pariser2011] mentions several risks about personalizing filters:

* Data collection and privacy
* Democracy
* **Information** (I will focus on this)
* Freedom
* Creativity
* Censorship
* Serendipity

## Information

* **Friendly world syndrome**: some of the most important problem don't reach our view at all
* We can miss major news and events
    * This was not possible with e.g. traditional newspapers
* In the filter bubble, the **public sphere** is less relevant
* Filters block **important, unpleasant things** that we *should* care about
    * Some topics will always be *not likable*: war, homelessness, poverty...
* *Relevance* is the only metric, and *importance* matters less
* The **choice** about what to read is no more in readers' hands
    * Contents inside your personal bubble aren't shown
* Often there's no way to disable the personalization


## Aspirational self and actual self
* Netflix queue example
* Personalization privileges the actual self
* Balanced information diet vs. information junk food
* Consuming information "near" to us is easy and pleasurable
* Consuming challenging info is difficult

# Facebook and Google's bubbles

## Facebook is too friendly!

Suppose that you are a Facebook user and you identify as a **liberal**.
You have both liberals and conservatives friends.

* In the News Feed, you get more posts which reflect what you like
* You may not see conservatives' stories at all, if you interact less with your conservative friends
* **Cross-cutting stories** (those different from our viewpoint) are less likely to reach us
    * How big is this phenomenon?

## Facebook study (1)

Facebook published a study [@bakshy2015] on *Science* about how likely are users to **view and interact with *cross-cutting content***.

![Exposure stages of news stories](images/ExposureStages.pdf){width=70%}

1. *Potential from network*: shared by friends
2. *Exposed*: effectively shown in users' News Feeds
3. *Selected*: clicked by the user

## Facebook study (2)

![Percentage of cross-cutting stories that gets through the stages of exposure.](images/CrossCuttingTotal.pdf)

## Facebook study (3)

Conclusions:

* The **friendship network** is the most important factor limiting the mix of content encountered in social media.
    * if I have only friends of the same political affiliation, the filter bubble is obvious
* **Individual choice** influences the exposure to cross-cutting content more than the News Feed filtering.
* The effect of **News Feed ranking** is limited:
    * -5% for liberals
    * -8% for conservatives

\alert{Thus, Facebook says, any "filter bubble" is not due to the News Feed selection algorithm.}

## Facebook study: criticism

### Limitations of the study

* The building of the **friendship network** is not independent from Facebook's algorithms.
    * Friends are only partly from "offline" connections
    * Facebook suggests both pages to like and new friends
* What about **sponsored content**?

### Methodological issues

* **Sample** of the study: people which declare their political affiliation.
    * may not be representative of the entire Facebook community
* Independent researchers can't access Facebook data and analyze it.

## The position is everything

* The **position** of a story in the News Feed is very important
    * the ranking algorithm may be used to promote some stories and not others
    * money can buy rankings!
    * even if the algorithm is "fair" *now*, what about the future?

![Click rate depends on the position of the story in the News Feed.](images/PositionNewsFeed.pdf){width=100%}


## Google: the classical relevance model
**Relevance** according to **PageRank**: a page is important if it's linked by important pages.

* Each incoming link, weighted by the relevance of its source, contributes to the relevance of the page
* **Universal ranking**: the PageRank algorithm is independent from the user who's querying

![Visual representation of PageRank](images/PageRank.png){width=60%}


## Google: relevance is now personal

**There is no "standard Google" anymore!**

* 2009: Google extended personalized search also to logged off users
* Google uses 57 *signals* to personalize the search results
* Example: a search for "Egypt" can yield different results:
    1. Protests and political issues
    2. Travel and vacation information
* More and more difficult to find *what we don't know we want*.





# Solutions and remedies

## Moralizing filters

* The Internet is showing off what we want to see, but not **what we need to see**
* What if one day Google could urge us to stop obsessing over Lady Gaga’s videos and instead pay attention to Darfur?
* Should someone decide what we need to see? [@morozov2011]
* This is **paternalism**.

## Making the algorithms transparent

* Even if they are public, we may not understand them
* ...

## On-off button

> “Google or Facebook could place a slider bar running from “only stuff I like” to “stuff other people like that I’ll probably hate” at the top of search results and the News Feed”

## Government oversight

Slide

# Counter-objections

## "But they are useful!"

* Filters help eliminating the noise in the information society
* ...

## "We can just shut them off"

Not so simple.

## They are just a natural evolution of technology

Maybe. Or maybe not!


# Conclusion

## Are personalizing filter harmful?

Yes, they can be harmful if the people are using them without knowing

* Though, they are a really good technology for many specific application
* Should not be used extensively for shaping public opinion
