# Introduction

## A provoking quote

> “A squirrel dying in front of your house may be more relevant to your interests right now than people dying in Africa.” – Mark Zuckerberg


## Definitions

> **Personalized search** refers to search experiences that are tailored specifically to an individual's interests by incorporating information about the individual beyond specific query provided. (Wikipedia)

> A **filter bubble** is a result of a personalized search in which a website algorithm selectively guesses what information a user would like to see based on information about the user (such as location, past click behavior and search history). (Wikipedia)


## Amazon's filter bubble
![A "real world" recommender system](images/recommender_shop.jpg){width=80%}


## Personalizing filters and information

In this presentation, I will show that:

> **Personalized search filters may be dangerous for information**, if they are applied without people knowing

* filters alter our perception of the world
* information given to citizens should be not filtered according to what they like
* democracy doesn't work without a good flow of information
* personalization filter are good for some specific domains, though


## The dangers of personalization
The book *The Filter Bubble* by Eli Pariser [@pariser2011] mentions several risks about personalizing filters:

* Data collection and privacy
* Democracy
* **Information** (I will focus on this)
* Freedom
* Creativity
* Censorship
* Serendipity


## Facebook is too friendly!

Suppose you are a liberal with both liberals and conservatives in your Facebook friends.

* You get more posts  which reflect what you like.
* Even if you have some conservative friends, you may not see their updates in the News Feed.
* What if everyone of us is in a personal echo chamber?
* How big is this problem?

## Facebook study (1)

Facebook published a study [@bakshy2015] on *Science* about how likely are users to view and interact with cross-cutting content.
The content may be:

1. shared by random others (random)
2. shared by friends (potential from network)
3. effectively shown in users' News Feeds (exposed)
4. clicked by users (selected)

## Facebook study (2)

![Average ideological diversity of content](images/CrossCuttingTotal.pdf){width=80%}

## Facebook study (3)

![Proportion of individuals with at least one cross-cutting story](images/CrossCuttingAtLeastOne.pdf){width=90%}

## Google: the classical relevance model
**Relevance** according to **PageRank**: a page is important if it's linked by important pages.

* Each incoming link, weighted by the relevance of its source, contributes to the relevance of the page
* **Universal ranking**: the PageRank algorithm is independent from the user who's querying

![Visual representation of PageRank](images/PageRank.png){width=60%}


## Google: relevance is now personal

**There is no "standard Google" anymore!**

* 2009: Google extended personalized search also to logged off users
* Google uses 57 *signals* to personalize the search results
* Example: a search for "Egypt" can yield different results:
    1. Protests and political issues
    2. Travel and vacation information
* More and more difficult to find *what we don't know we want*.


# Concerns about the Filter Bubble

## Freedom and autonomy

> “To be free, you have to be able not only to do what you want, but to know what's possible to do." (Eli Pariser)

* Loop: we read only what we're interested in because of the recommendation
* We get trapped in a local maximum of preference, and it's difficult to escape
* Self-fulfilling prophecy
* Inside the filter bubble we're unable to reach new ideas and point of view
* *Serendipity* is harmed


## Information

* Friendly world syndrome: some of the most important problem don't reach our view at all
* We can miss major news and events
* In the filter bubble, the public sphere is less relevant
* Filters block important, unpleasant things that we *should* care about
* Some topics will always be *not likable*, though they are important.
    * War, homelessness, poverty, ...
* Algorithms don't have embedded ethics
* Relevance shouldn't be the only metric


## Comparison with traditional media
* Pull (active) vs. push (passive) news fetching
* NYT journos aren't allowed to see how many clicks their articles got
* Curation by human gatekeepers vs. disintermediation
* Media that prioritize importance over relevance are useful


## Filter bubble is always on
* You don't decide (nor see) what's in it
* You don't see, from the inside, what's out.


## Aspirational self and actual self
* Netflix queue example
* Personalization privileges the actual self
* Balanced information diet vs. information junk food
* Consuming information "near" to us is easy and pleasurable
* Consuming challenging info is difficult


# Solutions and remedies

## Moralizing filters

* The Internet is showing off what we want to see, but not **what we need to see**
* What if one day Google could urge us to stop obsessing over Lady Gaga’s videos and instead pay attention to Darfur?
* Should someone decide what we need to see? [@morozov2011]
* This is **paternalism**.

## Making the algorithms transparent

* Even if they are public, we may not understand them
* ...

## On-off button

> “Google or Facebook could place a slider bar running from “only stuff I like” to “stuff other people like that I’ll probably hate” at the top of search results and the News Feed”

## Government oversight

Slide

# Counter-objections

## "But they are useful!"

* Filters help eliminating the noise in the information society
* ...

## "We can just shut them off"

Not so simple.

## They are just a natural evolution of technology

Maybe. Or maybe not!


# Conclusion

## Are personalizing filter harmful?

Yes, they can be harmful if the people are using them without knowing

* Though, they are a really good technology for many specific application
* Should not be used extensively for shaping public opinion

## References
